{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, glob, json, csv, subprocess, sys, re, operator\n",
    "from git import *\n",
    "from subprocess import Popen, PIPE\n",
    "from os import path\n",
    "import pandas as pd\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import plot, init_notebook_mode, iplot\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining repository and directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "userhome = os.path.expanduser('~')\n",
    "repository = userhome + r'/different-diff/dataset/openjpa/'\n",
    "analyze_dir = userhome + r'/different-diff/analyze/analyze_openjpa/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset number of files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "algorithms = ['histogram','minimal','myers','patience']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding a field key to combine 2 CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for j, alg in enumerate(algorithms):\n",
    "    with open(analyze_dir + \"03_file-diff/03_number_of_files/\" + alg + \"_total_bugline.csv\",'r') as csvinput:\n",
    "        with open(analyze_dir + \"04_annotate/05_matching_affectedfiles/temp_\" + alg + \"_bugline.csv\", 'w') as csvoutput:\n",
    "            writer = csv.writer(csvoutput, lineterminator='\\n')\n",
    "            reader = csv.reader(csvinput)\n",
    "\n",
    "            all = []\n",
    "            row = next(reader)\n",
    "            row.append('file_key')\n",
    "            all.append(row)\n",
    "            \n",
    "            num = 1\n",
    "            for row in reader:\n",
    "                row.append(row[0] + \"_\" + row[1] + \"-\" + row[2][:10] + \"_\" + row[4] + \"_\" + str(num))\n",
    "                all.append(row)\n",
    "                num += 1\n",
    "\n",
    "            writer.writerows(all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for j, alg in enumerate(algorithms):\n",
    "    with open(analyze_dir + \"04_annotate/04_grouping_affected_non-affected_files/\" + alg + \"_affected_files.csv\",'r') as csvinput:\n",
    "        with open(analyze_dir + \"04_annotate/05_matching_affectedfiles/temp_\" + alg + \"_aff_files.csv\", 'w') as csvoutput:\n",
    "            writer = csv.writer(csvoutput, lineterminator='\\n')\n",
    "            reader = csv.reader(csvinput)\n",
    "\n",
    "            all = []\n",
    "            row = next(reader)\n",
    "            row.append('file_key')\n",
    "            all.append(row)\n",
    "            \n",
    "            num = 1\n",
    "            for row in reader:\n",
    "                row.append(row[1] + \"_\" + row[3] + \"-\" + row[4] + \"_\" + row[2] + \"_\" + str(num))\n",
    "                all.append(row)\n",
    "                num += 1\n",
    "\n",
    "            writer.writerows(all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Joining bugline files and affected files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for k, algo in enumerate(algorithms):\n",
    "    filedatabug = pd.read_csv(analyze_dir + '04_annotate/05_matching_affectedfiles/temp_' + algo + '_bugline.csv')\n",
    "    filedataaff = pd.read_csv(analyze_dir + \"04_annotate/05_matching_affectedfiles/temp_\" + algo + \"_aff_files.csv\")\n",
    "\n",
    "    details = filedatabug.join(filedataaff.set_index('file_key')[['filenumber','affect_version?']], on='file_key')\n",
    "    details = details[details['number_of_buggyline'] != 0]\n",
    "    details.to_csv(analyze_dir + '05_finding_versions/02_files_affect/' + algo + '_joining_files.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separating affected and non-affected files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "headname = ['bug_id','bugintro_commitID','parent_id','filepath','filename','number_of_buggyline','file_key','filenumber','affect_version?']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for p, algor in enumerate(algorithms):\n",
    "    ds = pd.read_csv(analyze_dir + \"05_finding_versions/02_files_affect/\" + algor + \"_joining_files.csv\")\n",
    "    ds_affect = ds[headname][ds['affect_version?'] == 'Affect the version']\n",
    "    ds_nonaffect = ds[headname][ds['affect_version?'] == 'Does not affect the version']\n",
    "    \n",
    "    ds_affect = ds_affect[headname]\n",
    "    ds_nonaffect = ds_nonaffect[headname]\n",
    "    \n",
    "    ds_affect.to_csv(analyze_dir + \"05_finding_versions/02_files_affect/01_affected_files/\" + algor + \"_affected_files.csv\")\n",
    "    ds_nonaffect.to_csv(analyze_dir + \"05_finding_versions/02_files_affect/02_non-affected_files/\" + algor + \"_non-affected_files.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining validated file as a bug-introducing change file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for aa, algrt in enumerate(algorithms):\n",
    "    valid_file = pd.read_csv(analyze_dir + \"05_finding_versions/02_files_affect/01_affected_files/\" + algrt + \"_affected_files.csv\")\n",
    "    valid_file = (pd.to_numeric(valid_file['number_of_buggyline'], errors='coerce')\n",
    "                       .groupby(valid_file['filepath'])\n",
    "                       .sum()\n",
    "                       .to_frame()\n",
    "                       .add_prefix('total')\n",
    "                       .reset_index())\n",
    "\n",
    "    valid_file = valid_file.dropna(subset=['totalnumber_of_buggyline'])\n",
    "    valid_file = valid_file.sort_values('totalnumber_of_buggyline', ascending=False)\n",
    "    valid_file = valid_file[['filepath','totalnumber_of_buggyline']]\n",
    "    valid_file.to_csv(analyze_dir + \"05_finding_versions/03_filtering/02_valid_files/\" + algrt + \"_valid_files.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "uniq_hist_file = pd.read_csv(analyze_dir + \"05_finding_versions/03_filtering/02_valid_files/histogram_valid_files.csv\")\n",
    "uniq_hist_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total_bugline = []\n",
    "for ff, algorit in enumerate(algorithms):\n",
    "    file = pd.read_csv(analyze_dir + \"05_finding_versions/03_filtering/02_valid_files/\" + algorit + \"_valid_files.csv\")\n",
    "    file = file[['filepath','totalnumber_of_buggyline']]\n",
    "    filenum = file['filepath'].count()\n",
    "    bugnumber = 0\n",
    "    for z in range(0,len(file)):\n",
    "        bugnumber = bugnumber + file.iloc[z][1]\n",
    "    temp = [algorit, bugnumber, filenum]\n",
    "    total_bugline.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_bugline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding validated bug-introducing change commit id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for bb, algrt in enumerate(algorithms):\n",
    "    valid_bugintro = pd.read_csv(analyze_dir + \"05_finding_versions/02_files_affect/01_affected_files/\" + algrt + \"_affected_files.csv\")\n",
    "    valid_bugintro = (pd.to_numeric(valid_bugintro['number_of_buggyline'], errors='coerce')\n",
    "                       .groupby(valid_bugintro['bugintro_commitID'])\n",
    "                       .sum()\n",
    "                       .to_frame()\n",
    "                       .add_prefix('total')\n",
    "                       .reset_index())\n",
    "\n",
    "    valid_bugintro = valid_bugintro.dropna(subset=['totalnumber_of_buggyline'])\n",
    "    valid_bugintro = valid_bugintro.sort_values('totalnumber_of_buggyline', ascending=False)\n",
    "    valid_bugintro = valid_bugintro[['bugintro_commitID','totalnumber_of_buggyline']]\n",
    "    valid_bugintro.to_csv(analyze_dir + \"05_finding_versions/03_filtering/01_valid_bugintro_commitid/\" + algrt + \"_valid_bugintro_commitid.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numof_bugintro = []\n",
    "for ff, algorit in enumerate(algorithms):\n",
    "    file = pd.read_csv(analyze_dir + \"05_finding_versions/03_filtering/01_valid_bugintro_commitid/\" + algorit + \"_valid_bugintro_commitid.csv\")\n",
    "    file = file[['bugintro_commitID','totalnumber_of_buggyline']]\n",
    "    num = file['bugintro_commitID'].count()\n",
    "    temp = [algorit, num]\n",
    "    numof_bugintro.append(temp)\n",
    "numof_bugintro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding validated candidate bugfix commit id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "candidate_bugfix = pd.read_csv(analyze_dir + \"02_extracting_commitid/file_in_candidatebugfix_commitid.csv\")\n",
    "candidate_bugfix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter validated bug ids and files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#filter valid bug_id\n",
    "for mm, algo in enumerate(algorithms):\n",
    "    valid_bugid = pd.read_csv(analyze_dir + \"05_finding_versions/02_files_affect/01_affected_files/\" + algo + \"_affected_files.csv\")\n",
    "    valid_bugid = (pd.to_numeric(valid_bugid['number_of_buggyline'], errors='coerce')\n",
    "                       .groupby(valid_bugid['bug_id'])\n",
    "                       .sum()\n",
    "                       .to_frame()\n",
    "                       .add_prefix('total')\n",
    "                       .reset_index())\n",
    "\n",
    "    valid_bugid = valid_bugid.dropna(subset=['totalnumber_of_buggyline'])\n",
    "    valid_bugid = valid_bugid.sort_values('totalnumber_of_buggyline', ascending=False)\n",
    "    valid_bugid = valid_bugid[['bug_id','totalnumber_of_buggyline']]\n",
    "    valid_bugid.to_csv(analyze_dir + \"05_finding_versions/03_filtering/03_valid_bugfix_commitid/valid_bug_id/\" + algo + \"_valid_bugid.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Filter validated files and save into txt file\n",
    "for nn, alg in enumerate(algorithms):\n",
    "    uniq_file = pd.read_csv(analyze_dir + \"05_finding_versions/03_filtering/02_valid_files/\" + alg + \"_valid_files.csv\")\n",
    "    valid_file = []\n",
    "    for i in range(0, len(uniq_file)):\n",
    "        valid_file.append(uniq_file.iloc[i][0])\n",
    "    valid_file.sort()\n",
    "    \n",
    "    with open (analyze_dir + \"05_finding_versions/03_filtering/02_valid_files/txt/\" + alg + \"valid_file.txt\", mode=\"wt\", encoding=\"utf-8\") as myfile:\n",
    "        myfile.write('\\n'.join(valid_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Filter validated bug id and save into txt file\n",
    "for nn, alg in enumerate(algorithms):\n",
    "    uniq_bugid = pd.read_csv(analyze_dir + \"05_finding_versions/03_filtering/03_valid_bugfix_commitid/valid_bug_id/\" + alg + \"_valid_bugid.csv\")\n",
    "    valid_bugid = []\n",
    "    for i in range(0, len(uniq_bugid)):\n",
    "        valid_bugid.append(uniq_bugid.iloc[i][0])\n",
    "    valid_bugid.sort()\n",
    "    \n",
    "    with open (analyze_dir + \"05_finding_versions/03_filtering/03_valid_bugfix_commitid/valid_bug_id/txt/\" + alg + \"valid_bugid.txt\", mode=\"wt\", encoding=\"utf-8\") as myfile:\n",
    "        myfile.write('\\n'.join(valid_bugid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for aa, algor in enumerate(algorithms):\n",
    "    with open(analyze_dir + \"02_extracting_commitid/file_in_candidatebugfix_commitid.csv\",'r') as fin, open (analyze_dir + \"05_finding_versions/03_filtering/03_valid_bugfix_commitid/\" + algor + \"_validbugfix_commitid.csv\",'w') as fout:\n",
    "        writers = csv.writer(fout, delimiter=',')\n",
    "        c_name = ['validated_bug_id','validated_bugfix_commitid','validated_file']\n",
    "        writers.writerow(c_name)\n",
    "        \n",
    "        val_bugid = open(analyze_dir + \"05_finding_versions/03_filtering/03_valid_bugfix_commitid/valid_bug_id/txt/\" + algor + \"valid_bugid.txt\")\n",
    "        val_bugid = val_bugid.read().split(\"\\n\")\n",
    "        \n",
    "        val_file = open(analyze_dir + \"05_finding_versions/03_filtering/02_valid_files/txt/\" + algor + \"valid_file.txt\")\n",
    "        val_file = val_file.read().split(\"\\n\")\n",
    "        \n",
    "        for row in csv.reader(fin, delimiter=','):\n",
    "            if row[0] in val_bugid:\n",
    "                if row[2] in val_file: \n",
    "                    writers.writerow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing duplicate bugfix_commit_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for bb, algrt in enumerate(algorithms):\n",
    "    valid_bugfix = pd.read_csv(analyze_dir + \"05_finding_versions/03_filtering/03_valid_bugfix_commitid/\" + algrt + \"_validbugfix_commitid.csv\")\n",
    "    valid = []\n",
    "    for row in range(0,len(valid_bugfix)):\n",
    "        if valid_bugfix.iloc[row][1] not in valid:\n",
    "            valid.append(valid_bugfix.iloc[row][1])\n",
    "    with open (analyze_dir + \"05_finding_versions/03_filtering/03_valid_bugfix_commitid/distinct_bugfix_commitid/\" + algrt + \"valid_bugfix_commitid.txt\", mode=\"wt\", encoding=\"utf-8\") as myfile:\n",
    "        myfile.write('\\n'.join(valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_bugfix = open(analyze_dir + \"05_finding_versions/03_filtering/03_valid_bugfix_commitid/distinct_bugfix_commitid/histogramvalid_bugfix_commitid.txt\")\n",
    "val_bugfix = val_bugfix.read().split(\"\\n\")\n",
    "val_bugfix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numof_bugfixcid = []\n",
    "for cc, algrt in enumerate(algorithms):\n",
    "    txtfile = open(analyze_dir + \"05_finding_versions/03_filtering/03_valid_bugfix_commitid/distinct_bugfix_commitid/\" + algrt + \"valid_bugfix_commitid.txt\")\n",
    "    txtfile = txtfile.read().split(\"\\n\")\n",
    "    temp = [algrt, len(txtfile)]\n",
    "    numof_bugfixcid.append(temp)\n",
    "numof_bugfixcid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merging the list of total_bugline and numof_bugintro\n",
    "d = dict(numof_bugintro)\n",
    "totbug_numbugintro = [i + [d[i[0]]] for i in total_bugline]\n",
    "totbug_numbugintro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merging the list of totbug_numbugintro and numof_bugfixcid\n",
    "d = dict(numof_bugfixcid)\n",
    "join_list = [i + [d[i[0]]] for i in totbug_numbugintro]\n",
    "join_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_matrix = [list(i) for i in zip(*join_list)]\n",
    "label = ['algorithm','#validated_buglines', '#validated_files', '#validated_bug-intro_commitID', '#validated_bugfix_commitID']\n",
    "for s, lbl in enumerate(label):\n",
    "    result_matrix[s].insert(0, lbl)\n",
    "result_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(analyze_dir + \"06_statistics/final_stats.csv\",\"w\") as finalfile:\n",
    "    writers = csv.writer(finalfile, delimiter=\",\")\n",
    "    colnames = ['item','histogram', 'minimal','myers','patience']\n",
    "    writers.writerow(colnames)\n",
    "    for row in result_matrix[1:]:\n",
    "        writers.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_stat = pd.read_csv(analyze_dir + \"06_statistics/final_stats.csv\")\n",
    "data_stat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing number of validated buglines in graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = go.Bar(\n",
    "    x = ['histogram','minimal','myers','patience'],\n",
    "    y = [data_stat.iloc[0][1],data_stat.iloc[0][2],data_stat.iloc[0][3],data_stat.iloc[0][4]],\n",
    "    text = [data_stat.iloc[0][1],data_stat.iloc[0][2],data_stat.iloc[0][3],data_stat.iloc[0][4]],\n",
    "    textposition = 'auto',\n",
    "    marker = dict(\n",
    "        color = 'rgb(150,255,200)',\n",
    "        line = dict(\n",
    "            color='rgb(8,48,107)',\n",
    "            width=1.5),\n",
    "    ),\n",
    "    opacity=0.6\n",
    ")\n",
    "\n",
    "data = [hist]\n",
    "layout = go.Layout(\n",
    "    title = \"Number of validated buglines in OPENJPA Project\"\n",
    ")\n",
    "\n",
    "init_notebook_mode(connected=True)\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "iplot(fig, show_link=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing number of validated files, bug-intro and bug-fix commit in graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numhist = go.Bar(\n",
    "    x = ['validated_files','validated_bug-intro_commitID','validated_bugfix_commitID'],\n",
    "    y = data_stat['histogram'].iloc[1:],\n",
    "    text = data_stat['histogram'].iloc[1:],\n",
    "    textposition = 'auto',\n",
    "    name = 'Histogram'\n",
    ")\n",
    "\n",
    "nummin = go.Bar(\n",
    "    x = ['validated_files','validated_bug-intro_commitID','validated_bugfix_commitID'],\n",
    "    y = data_stat['minimal'].iloc[1:],\n",
    "    text = data_stat['minimal'].iloc[1:],\n",
    "    textposition = 'auto',\n",
    "    name = 'minimal'\n",
    ")\n",
    "\n",
    "nummyers = go.Bar(\n",
    "    x = ['validated_files','validated_bug-intro_commitID','validated_bugfix_commitID'],\n",
    "    y = data_stat['myers'].iloc[1:],\n",
    "    text = data_stat['myers'].iloc[1:],\n",
    "    textposition = 'auto',\n",
    "    name = 'Myers'\n",
    ")\n",
    "\n",
    "numpat = go.Bar(\n",
    "    x = ['validated_files','validated_bug-intro_commitID','validated_bugfix_commitID'],\n",
    "    y = data_stat['patience'].iloc[1:],\n",
    "    text = data_stat['patience'].iloc[1:],\n",
    "    textposition = 'auto',\n",
    "    name = 'Patience'\n",
    ")\n",
    "\n",
    "data = [numhist, nummin, nummyers, numpat]\n",
    "layout = go.Layout(\n",
    "    title = \"Number of validated files, bug-intro commit id and bugfix commit id in OPENJPA Project\"\n",
    ")\n",
    "\n",
    "init_notebook_mode(connected=True)\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "iplot(fig, show_link=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
